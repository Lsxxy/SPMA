############## The train & val & test set root ##############
train_file: [/root/Documents/code/HarMA_with_RS5M/data/finetune/rsicd_train.json]    # Training data file
val_file: /root/Documents/code/HarMA_with_RS5M/data/finetune/rsicd_val.json       # Validation data file
test_file: /root/Documents/code/HarMA_with_RS5M/data/finetune/rsicd_test.json      # Testing data file
image_root: /root/Documents/code/HarMA_with_RS5M/data/images/rsicd        # Root directory for images

image_res: 224  # no need modify
patch_size: 32   #if use swin, set the patch_size to 32, else 16

############## Text encoder setting ##############
text_config: configs/config_bert.json
text_encoder: data/bert-base-uncased

################ Training setting ################
#== no need revise in general
batch_size_train: 128
batch_size_test: 128
batch_size_test_text: 128

embed_dim: 512
temp1: 0.07
temp2: 0.07
SPMA: True 
use_prompt: True
if_evaluation: False
save_epoch: True
save_num_epoch: 10

############## Other Settings ##############
optimizer: {opt: adamW, lr: 0.0004, weight_decay: 0.04, lr_mult: 2}
schedular: {sched: linear, lr: 0.0004, epochs: 5, num_warmup_steps: 0.1} # need to set the epoches, if needed, also lr

################ Model setting  #######################################################################################
#== 1. Representation Alignment, RA                                                                                 ####
use_affil_loss: true # use affil loss
use_triplet_loss: false
center_factor: 1 # if use affil loss, set the center factor                                                      ####
# indistinct_margin: 0.01                                                                                           ####
                                                                                                                    ####
#== 2. Vision Instruction Representation, VIR                                                                       ####
filter_size: 40 # modify the filter size of vision instruction representation                                       ####
instru_num: 2 # 2                                                                                                       #=##
#== 3. Language Cycle Attention,  LCA                                                                               ####
cycle_num: 3 # 3 # how many times of cycle attention                                                                    ####
                                                                                                                    ####
#== 4. the SA & CA parameter (include VIR and LCA Module)                                                           ####
dropout_r: 0.2                                                                                                      #=##
head: 8                                                                                                             #=##
########################################################################################################################

